# デスクトップ製品をクラウドサービスに移行するという事

## はじめに

2010年代頃から多くのデスクトップアプリケーションがクラウドサービス化しています。
この記事では、エンジニア目線ではどのようなものを作るのかという事にフォーカスしたいと思います。

典型的なパターンでは既存のアプリケーションのデータを例えばZIPファイルでエクスポートし、クラウドサービス側にインポートすることが必要です。

今回はエクスポートツールが出力したファイルをどのように処理する仕掛けを構築するのか、というところをまとめます。

システム移行の典型的なパイプライン
全体のフローとパイプラインはこのようになります。

![image](https://github.com/user-attachments/assets/37bc22d8-30cd-46e2-ad59-9f39180b0932)

## export
基本的にクライアント側でスタンドアロンタイプのアプリケーションは、独自形式のデータを持つことが多く、新サービスへの移行をするためにはエクスポートツールを作る必要があります。

- デスクトップ製品とは異なるデータ構造を持つことがあり得るため、なるべく平易な形でのエクスポートします
- CSV、XML、JSONなど一般的なテキストフォーマットが良いでしょう
- フォーマットは平易でも、暗号化するべきです
- パスワード付きとするのは重要です。パスワードを知る人にしか作業が出来ないようになっているなら、セキュリティ面のリスクが低減します
- エクスポートツールのバージョン、改竄されていないことを確認するための情報が必要です

ここまではデスクトップ製品の開発者が実装し、仕様書を作る必要があるでしょう

## unzip
ファイルとパスワードを受け取ったら、ファイルを展開します。
その際、いくつかのチェックを済ませるべきです。

- エクスポートしたときのパスワードを用いているか
- 必要なファイルを内包していること

NGである場合、**処理継続不能なエラー**を返却します。
また複数ファイルを受け入れる仕様の場合、以下のチェックを追加すべきです。

同じファイルを2回以上アップロードしていないか（ファイルの削除をユーザーに促す）

## decrypt
ファイルを展開したら暗号を復号化します。

重要：この処理は少なくともサーバー側で実行しなければいけません。復号化キーをブラウザまでもっていくことは重大なセキュリティ問題となります。

その際、いくつかのチェックを済ませるべきです。

- 復号用のキーで展開できなかった場合

NGである場合、**処理継続不能なエラー**を返却します。

暗号を復号化した後、最初にすべき判定は以下の通りです

- エクスポートツールのバージョンはサービスがサポート中のものか
- エクスポート後にユーザーなどによって変更されていないか

NGである場合、**処理継続不能なエラー**を返却します。

## transform
移行するデータ本体を開発言語に適した形へ変換します。（例はTypeScriptです）

CSV/XML/JSONなどのテキストのフォーマットがとなっているはずです。
まずは、読み込んだ情報を平易な型に変換します。
XML/JSONの場合は大抵シンプルなデシリアライズで済みます。
CSVなどでヘッダー行を用いる場合、変数として相応しくない文字列が使われている可能性があります。

以下は従業員.csvの例です
```
"Code","氏名","生年月日"
”E0001”,”山本太郎”,"1972-11-12"
"E0002",”山本次郎”,"1975-1-30"
```
こういったものは開発言語に相応しいクラス名とフィールド名に変換する処理を作る必要があります。

```typescript
const employeeTransform: {[fieldName: string]: csvColumnName} = {
  employeeCode: "Code";
  name: "氏名";
  birthDate: "生年月日";
};

// rows にCSVの各行を読み込んでいるものとする。こんな風に書けるようにしたい。
const employees: {[fieldName: string]: value} = 
  rows.map((row) => Pipeline.transform(employeeTransform, row));
```
`Pipeline.transform` は `fieldName` と `csvColumnName` を関連付けた情報をもとに、CSVの行を変換する汎用的な関数として実装する必要があります。

## validation
移行先のシステムが受け入れられない部分、あるいはデータを変換してしまう部分について考えます。

- 受け入れられない部分：error
- 変換が必要で完全な移行にはならない部分：warning (事後通知)

例えば、上記の 従業員.csv の場合、受け入れ先のシステムの仕様によっては、Codeの重複はエラー扱いとなるかもしれません。
デスクトップ版とクラウド版で仕様が異なる場合、綿密に仕様差異の埋め方を検討する必要があります。
必要に応じて、これらを編集する画面を用意する必要があるでしょう。

移行ファイルが数十種、カラム数で数百を数えるような規模のデータ構造では、この部分を共通化してシステマチックに扱う必要があります。こんな風に書けたらかなり分かりやすいかもしれません

```
// 雰囲気コードです
class EmployeeValidator extends ValidatorBase<CsvEmployee, Employee> {
  constractor({
    rows: [requred: false] // 1行もないデータを受け入れる
    error: {
      employeeCode: [required, unique];
      name: [required];
    },
    warning: {
      birthDate: [range({ min: new Date('1900-01-01'), max: Date.now()})] 
      //↑クラウド版は 1900年～現在以外を受け入れない場合
    }
  }) {}
}
```

### errorが出た場合
移行処理を中断しなければなりません。
ただしこの段階では1つのエラーで中断するのではなく、できるだけ多くのエラーを収集する必要があります。
1つエラーを直したら次のエラーが出る、という形には決してしてはなりません。

> このValidation処理以後、ネットワークやインフラのエラー以外の事情でエラーを出すことは一切許されません。

### warningが出力された場合
移行処理後にユーザーへ伝える必要があります。この情報は保存しておきます。

## データ編集画面
validationでエラーとなる部分を修正する機会をユーザーに提供します。
ここで編集した内容は、続く merge 処理で元データを上書きします。
その際の結果が validation エラーとならないようにする必要があります。

## merge
ユーザーがデータ編集画面上で修正を加えたデータを、元のデータに対して上書きします。
データ編集画面上で修正した場合、validationのエラーが全て出ない状態になる必要があります。

## convert / JSON serialize
validationが完了しているものを、APIが受け入れられる型に変換します。
この時の変換結果はEntity型とほぼ等価な内容にします。

> ここで例外が生じるケースはバグとして扱います。

## JSON deserialize（Schema Validation）
JSONをデシリアライズして、ZodなどでValidatonします。

- 正常なアクセスにおいて、ここでエラーが出ることはありません。
- 不正なアクセスに対する防御として、ここにバリデーション処理と認証認可による保護は必要です。

この段階で、idの生成とリレーションのみ行えばDBにそのまま出力できる型となるのが望ましいと言えます。

## ORMapper (Database updating)
OMマッパーを用いてDBに出力します。

## 移行結果画面
処理終了後、移行結果画面から warning の内容を取得できるようにします。

## まとめ
変換の精密さはもちろん必要ですが「エラーと警告をどこでどのように整理するか」が大きなポイントとなります。

また、これは典型的な例を小さく取り扱ったもので、多くの場合これより複雑になります。
しかし、連なれば大きな処理を、小さな責務の単位に区切ることで簡素化を狙うものとなります。
責務の単位で処理を分割・定型化することで、この種の複雑さと戦うことはかなり容易になります。

以上、エンジニア目線の製品移行でした。
